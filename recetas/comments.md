## Scrapping
 
The scrapping code is very well written and well organized. However, there are a few key issues that could cause the website to block you, or worse affect how legitimate users are using the website.
- The most important thing is that you should avoid causing DoS on the website. The way your code is set up, you loop through each recipe URL sending a request for each in every iteration, if you get an error response you wait 5 seconds. The problem with that is that you're sending multiple requests one after the other, which could cause your IP to be blocked, or worse, crash the website. A good way to avoid that is to add an interval between each request, ideally, a random one (e.g between 1 and 5 seconds) to emulate the behavior of a human user.
 - Hardcoded cookies like you're doing in the header in `Argentinasspider()` are a bad idea. Cookies are generally temporary and can be associated with your IP address. So if you're using this same cookie, say a month later from a cafe, you could be blocked from the website. If you need to use cookies to access the website, I would suggest you get a new one every session from the website response.
 
## Coding in general
 
In terms of general comments, your code looks really good overall! I do have some suggestions in terms of reproducibility and usability:
 - Hardcoded file paths are bad. These make it a lot harder for someone else (or yourself if your folder structure changes) to run your code. There are several alternatives to do this, what I would suggest is making sure your project sets up a folder structure with relative file paths within the project folder. At the very least create a variable in the beginning of each script to store file paths so it is easy to change.
 - Another thing is that there are too many functions alternating with execution. That can be ok if done in moderation, but I would suggest saving functions in a modules folder and importing them with something `from modules.scraping import (...)`. This way the execution code is clean and really easy to tell what's going on.
 - It is a DIME standard to set up a master script that executes other components of your code for research reproducibility and transparency purposes. That can be a bit finicky to set up using python and jupyter especially, but you could use the modules architecture and have just one notebook where all the execution happens and you set up all global variables, i.e file paths URLs, and other parameters there.
 - I think there should be a few more comments on each step of the code. There is a certain philosophy that good code is self-evident and it doesn't need comments. That may be true for other aspects of CS, but when processing data (be it structured like a CSV or unstructured like a webpage) there are a lot of arbitrary decisions you have to make to adapt your code to the data. So explaining why you're doing something really helps to understand the code without having to dig too much into the data.
 - On the docstrings, I would add few things: (1) a short description of what that function is supposed to do; (2) what class/type is expected from the inputs; and (3) if not clear a short description of what it outputs.


